{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain_tumour_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nW7qqlNPuSK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img \n",
        "import zipfile\n",
        "import cv2\n",
        "from skimage import io\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEQ_pjAqW0a3"
      },
      "source": [
        "brain_df = pd.read_csv('data_mask.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRsODQ2vXNss"
      },
      "source": [
        "\n",
        "#to check information present in the data set \n",
        "brain_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88NtZ5UmXuto"
      },
      "source": [
        "brain_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nRM_7mcXwsK"
      },
      "source": [
        "\n",
        "brain_df.mask_path[1] #path of the brain MRI image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b86WPoLs1w"
      },
      "source": [
        "brain_df.image_path[1]# image path of the brain MRI image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPm2QvLqL4x4"
      },
      "source": [
        "brain_df['mask'].value_counts() #0 represents the images that are classified as healthy 1 represents paitents that have tumour prsent in them "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJcNtdh4MQJq"
      },
      "source": [
        "brain_df['mask'].value_counts().index #checks the index of the brain mask column in the dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B25-TRKNBQU"
      },
      "source": [
        "import plotly.graph_objects as go #plottly is used to for plotting bar graph, histogram and many more \n",
        "#or use the sns but it is not as interactive as plottly \n",
        "fig = go.Figure([go.Bar(x = brain_df['mask'].value_counts().index, y = brain_df['mask'].value_counts())])\n",
        "fig.update_traces(marker_color = 'rgb(0,200,0)', marker_line_color = 'rgb(0,255,0)',\n",
        "                  marker_line_width = 7, opacity = 0.6)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMM-J6AnN8lv"
      },
      "source": [
        "brain_df.mask_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41CGdbkNbmKS"
      },
      "source": [
        "brain_df.image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo9CpkvNcaLt"
      },
      "source": [
        "plt.imshow(cv2.imread(brain_df.mask_path[623]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICAKPSqKZhVX"
      },
      "source": [
        "plt.imshow(cv2.imread(brain_df.image_path[623]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NUM7GJcHkWa"
      },
      "source": [
        "cv2.imread(brain_df.mask_path[623]).max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llSsHSK7Hn0f"
      },
      "source": [
        "cv2.imread(brain_df.mask_path[623]).min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnXf53EsHqQi"
      },
      "source": [
        "# Basic visualizations: Visualize the images (MRI and Mask) in the dataset separately \n",
        "import random\n",
        "fig, axs = plt.subplots(6,2, figsize=(16,32))\n",
        "count = 0\n",
        "for x in range(6):\n",
        "  i = random.randint(0, len(brain_df)) # select a random index\n",
        "  axs[count][0].title.set_text(\"Brain MRI\") # set title\n",
        "  axs[count][0].imshow(cv2.imread(brain_df.image_path[i])) # show MRI \n",
        "  axs[count][1].title.set_text(\"Mask - \" + str(brain_df['mask'][i])) # plot title on the mask (0 or 1)\n",
        "  axs[count][1].imshow(cv2.imread(brain_df.mask_path[i])) # Show corresponding mask\n",
        "  count += 1\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYAUjj0_Hq4M"
      },
      "source": [
        "#plotting 12 random images performing sanity check \n",
        "count = 0\n",
        "fig, axs = plt.subplots(12, 3, figsize = (20, 50))\n",
        "for i in range(len(brain_df)):\n",
        "  if brain_df['mask'][i] ==1 and count <12:\n",
        "    img = io.imread(brain_df.image_path[i])\n",
        "    axs[count][0].title.set_text('Brain MRI')\n",
        "    axs[count][0].imshow(img)\n",
        "\n",
        "    mask = io.imread(brain_df.mask_path[i])\n",
        "    axs[count][1].title.set_text('Mask')\n",
        "    axs[count][1].imshow(mask, cmap = 'gray')\n",
        "\n",
        "    \n",
        "    img[mask == 255] = (255, 0, 0)\n",
        "    axs[count][2].title.set_text('MRI with Mask')\n",
        "    axs[count][2].imshow(img)\n",
        "    count+=1\n",
        "\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJD0VaY5H39h"
      },
      "source": [
        "Training classifier model to detect brain  tumour "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piddHNoBH0bR"
      },
      "source": [
        "# Drop the patient id column\n",
        "brain_df_train = brain_df.drop(columns = ['patient_id'])\n",
        "brain_df_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKsVgrSmIBf7"
      },
      "source": [
        "# Convert the data in mask column to string format, to use categorical mode in flow_from_dataframe\n",
        "# You will get this error message if you comment out the following code line:\n",
        "# TypeError: If class_mode=\"categorical\", y_col=\"mask\" column values must be type string, list or tuple.\n",
        "brain_df_train['mask'] = brain_df_train['mask'].apply(lambda x: str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t57KMunHIDjN"
      },
      "source": [
        "brain_df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCujEErzIFlV"
      },
      "source": [
        "# split the data into train and test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(brain_df_train, test_size = 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdzzxKfgIIha"
      },
      "source": [
        "# create a image generator\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create a data generator which scales the data from 0 to 1 and makes validation split of 0.15\n",
        "datagen = ImageDataGenerator(rescale=1./255., validation_split = 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_pPHfasIKln"
      },
      "source": [
        "train_generator=datagen.flow_from_dataframe(\n",
        "dataframe=train,\n",
        "directory= './',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "subset=\"training\",\n",
        "batch_size=16,\n",
        "shuffle=True,\n",
        "class_mode=\"categorical\",\n",
        "target_size=(256,256))\n",
        "\n",
        "\n",
        "valid_generator=datagen.flow_from_dataframe(\n",
        "dataframe=train,\n",
        "directory= './',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "subset=\"validation\",\n",
        "batch_size=16,\n",
        "shuffle=True,\n",
        "class_mode=\"categorical\",\n",
        "target_size=(256,256))\n",
        "\n",
        "# Create a data generator for test images\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=test,\n",
        "directory= './',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "batch_size=16,\n",
        "shuffle=False,\n",
        "class_mode='categorical',\n",
        "target_size=(256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_q9Kyp-INDg"
      },
      "source": [
        "# Getting the ResNet50 base model\n",
        "basemodel = ResNet50(weights = 'imagenet', include_top = False, input_tensor = Input(shape=(256, 256, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeJq3RoTIQMu"
      },
      "source": [
        "basemodel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQX-qLBnISdz"
      },
      "source": [
        "# freeze the model weights\n",
        "\n",
        "for layer in basemodel.layers:\n",
        "  layers.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbTYnT3iIVqa"
      },
      "source": [
        "# Add classification head to the base model\n",
        "\n",
        "headmodel = basemodel.output\n",
        "headmodel = AveragePooling2D(pool_size = (4,4))(headmodel)\n",
        "headmodel = Flatten(name= 'flatten')(headmodel)\n",
        "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "headmodel = Dropout(0.3)(headmodel)#\n",
        "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "headmodel = Dropout(0.3)(headmodel)\n",
        "#headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "#headmodel = Dropout(0.3)(headmodel)\n",
        "headmodel = Dense(2, activation = 'softmax')(headmodel)\n",
        "\n",
        "model = Model(inputs = basemodel.input, outputs = headmodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WyhkM_-IYBK"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGQiiVNuIcTc"
      },
      "source": [
        "# compile the model\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pf3NmNbIdF3"
      },
      "source": [
        "# use early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath=\"classifier-resnet-weights.hdf5\", verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9kBq4mYIfCn"
      },
      "source": [
        "history = model.fit(train_generator, steps_per_epoch= train_generator.n // 16, epochs = 1, validation_data= valid_generator, validation_steps= valid_generator.n // 16, callbacks=[checkpointer, earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhnSA0DwIhGE"
      },
      "source": [
        "# save the model architecture to json file for future use\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"classifier-resnet-model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI6yMyl1ImKB"
      },
      "source": [
        "ASSESING THE TRAINING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "winggZVTIjRq"
      },
      "source": [
        "# Load pretrained model (instead of training the model for 1+ hours) \n",
        "with open('resnet-50-MRI.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "# load the model  \n",
        "model = tf.keras.models.model_from_json(json_savedModel)\n",
        "model.load_weights('weights.hdf5')\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RU3KcoTIllz"
      },
      "source": [
        "# make prediction\n",
        "\n",
        "test_predict = model.predict(test_generator, steps = test_generator.n // 16, verbose =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PhdXQkwIv9P"
      },
      "source": [
        "test_predict.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnN6l48UI146"
      },
      "source": [
        "test_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlks0tDYI3mC"
      },
      "source": [
        "# Obtain the predicted class from the model prediction\n",
        "predict = []\n",
        "\n",
        "for i in test_predict:\n",
        "  predict.append(str(np.argmax(i)))\n",
        "\n",
        "predict = np.asarray(predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xirIZLlI4MZ"
      },
      "source": [
        "predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q62kDlaxI59-"
      },
      "source": [
        "# since we have used test generator, it limited the images to len(predict), due to batch size\n",
        "original = np.asarray(test['mask'])[:len(predict)]\n",
        "len(original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L7efdm8I8fG"
      },
      "source": [
        "# Obtain the accuracy of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(original, predict)\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn2AqlWnI-T7"
      },
      "source": [
        "# plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(original, predict)\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRVKs62JAcP"
      },
      "source": [
        "#CLASSIFICATION REPORT\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(original, predict, labels = [0,1])\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPwOSPnSJIoi"
      },
      "source": [
        "# Get the dataframe containing MRIs which have masks associated with them.\n",
        "brain_df_mask = brain_df[brain_df['mask'] == 1]\n",
        "brain_df_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujqn9qK4JL5K"
      },
      "source": [
        "# split the data into train and test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val = train_test_split(brain_df_mask, test_size=0.15)\n",
        "X_test, X_val = train_test_split(X_val, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdnboa1SJOOn"
      },
      "source": [
        "# create separate list for imageId, classId to pass into the generator\n",
        "\n",
        "train_ids = list(X_train.image_path)\n",
        "train_mask = list(X_train.mask_path)\n",
        "\n",
        "val_ids = list(X_val.image_path)\n",
        "val_mask= list(X_val.mask_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9VCAL-vJO6P"
      },
      "source": [
        "# Utilities file contains the code for custom loss function and custom data generator\n",
        "from utilities import DataGenerator\n",
        "\n",
        "# create image generators\n",
        "\n",
        "training_generator = DataGenerator(train_ids,train_mask)\n",
        "validation_generator = DataGenerator(val_ids,val_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "galGJWIDJQ1s"
      },
      "source": [
        "def resblock(X, f):\n",
        "  \n",
        "\n",
        "  # make a copy of input\n",
        "  X_copy = X\n",
        "\n",
        "  # main path\n",
        "  # Read more about he_normal: https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
        "\n",
        "  X = Conv2D(f, kernel_size = (1,1) ,strides = (1,1),kernel_initializer ='he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f, kernel_size = (3,3), strides =(1,1), padding = 'same', kernel_initializer ='he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "\n",
        "  # Short path\n",
        "  # Read more here: https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
        "\n",
        "  X_copy = Conv2D(f, kernel_size = (1,1), strides =(1,1), kernel_initializer ='he_normal')(X_copy)\n",
        "  X_copy = BatchNormalization()(X_copy)\n",
        "\n",
        "  # Adding the output from main path and short path together\n",
        "\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgC6XJthJTBn"
      },
      "source": [
        "# function to upscale and concatenate the values passsed\n",
        "def upsample_concat(x, skip):\n",
        "  x = UpSampling2D((2,2))(x)\n",
        "  merge = Concatenate()([x, skip])\n",
        "\n",
        "  return merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI9_OhmgJXii"
      },
      "source": [
        "input_shape = (256,256,3)\n",
        "\n",
        "# Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "# Stage 1\n",
        "conv1_in = Conv2D(16,3,activation= 'relu', padding = 'same', kernel_initializer ='he_normal')(X_input)\n",
        "conv1_in = BatchNormalization()(conv1_in)\n",
        "conv1_in = Conv2D(16,3,activation= 'relu', padding = 'same', kernel_initializer ='he_normal')(conv1_in)\n",
        "conv1_in = BatchNormalization()(conv1_in)\n",
        "pool_1 = MaxPool2D(pool_size = (2,2))(conv1_in)\n",
        "\n",
        "# Stage 2\n",
        "conv2_in = resblock(pool_1, 32)\n",
        "pool_2 = MaxPool2D(pool_size = (2,2))(conv2_in)\n",
        "\n",
        "# Stage 3\n",
        "conv3_in = resblock(pool_2, 64)\n",
        "pool_3 = MaxPool2D(pool_size = (2,2))(conv3_in)\n",
        "\n",
        "# Stage 4\n",
        "conv4_in = resblock(pool_3, 128)\n",
        "pool_4 = MaxPool2D(pool_size = (2,2))(conv4_in)\n",
        "\n",
        "# Stage 5 (Bottle Neck)\n",
        "conv5_in = resblock(pool_4, 256)\n",
        "\n",
        "# Upscale stage 1\n",
        "up_1 = upsample_concat(conv5_in, conv4_in)\n",
        "up_1 = resblock(up_1, 128)\n",
        "\n",
        "# Upscale stage 2\n",
        "up_2 = upsample_concat(up_1, conv3_in)\n",
        "up_2 = resblock(up_2, 64)\n",
        "\n",
        "# Upscale stage 3\n",
        "up_3 = upsample_concat(up_2, conv2_in)\n",
        "up_3 = resblock(up_3, 32)\n",
        "\n",
        "# Upscale stage 4\n",
        "up_4 = upsample_concat(up_3, conv1_in)\n",
        "up_4 = resblock(up_4, 16)\n",
        "\n",
        "# Final Output\n",
        "output = Conv2D(1, (1,1), padding = \"same\", activation = \"sigmoid\")(up_4)\n",
        "\n",
        "model_seg = Model(inputs = X_input, outputs = output )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDADyb3yJZp9"
      },
      "source": [
        "model_seg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5HClHVTJcal"
      },
      "source": [
        "# Utilities file contains the code for custom loss function and custom data generator\n",
        "\n",
        "from utilities import focal_tversky, tversky_loss, tversky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUDbH7LvJeZO"
      },
      "source": [
        "# Compile the model\n",
        "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
        "model_seg.compile(optimizer = adam, loss = focal_tversky, metrics = [tversky])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9qOP15xJg4r"
      },
      "source": [
        "# use early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "# save the best model with lower validation loss\n",
        "checkpointer = ModelCheckpoint(filepath=\"ResUNet-weights.hdf5\", verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uC1SqqSJjOG"
      },
      "source": [
        "history = model_seg.fit(training_generator, epochs = 1, validation_data = validation_generator, callbacks = [checkpointer, earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdE9Ov8IJj6p"
      },
      "source": [
        "# save the model architecture to json file for future use\n",
        "\n",
        "model_json = model_seg.to_json()\n",
        "with open(\"ResUNet-model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ggvqXsJn92"
      },
      "source": [
        "from utilities import focal_tversky, tversky_loss, tversky\n",
        "\n",
        "with open('ResUNet-MRI.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "# load the model architecture \n",
        "model_seg = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_seg.load_weights('weights_seg.hdf5')\n",
        "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
        "model_seg.compile(optimizer = adam, loss = focal_tversky, metrics = [tversky])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhVs7ZP0Jolg"
      },
      "source": [
        "# Utilities file contains the code for custom loss function and custom data generator\n",
        "from utilities import prediction\n",
        "\n",
        "# making prediction\n",
        "image_id, mask, has_mask = prediction(test, model, model_seg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW31Jh5vJqut"
      },
      "source": [
        "# creating a dataframe for the result\n",
        "df_pred = pd.DataFrame({'image_path': image_id,'predicted_mask': mask,'has_mask': has_mask})\n",
        "df_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4h2GGsjJuDq"
      },
      "source": [
        "# Merge the dataframe containing predicted results with the original test data.\n",
        "df_pred = test.merge(df_pred, on = 'image_path')\n",
        "df_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks9F6x0kJuoC"
      },
      "source": [
        "count = 0 \n",
        "fig, axs = plt.subplots(10, 5, figsize=(30, 50))\n",
        "for i in range(len(df_pred)):\n",
        "  if df_pred['has_mask'][i] == 1 and count < 10:\n",
        "    # read the images and convert them to RGB format\n",
        "    img = io.imread(df_pred.image_path[i])\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    axs[count][0].title.set_text(\"Brain MRI\")\n",
        "    axs[count][0].imshow(img)\n",
        "\n",
        "    # Obtain the mask for the image \n",
        "    mask = io.imread(df_pred.mask_path[i])\n",
        "    axs[count][1].title.set_text(\"Original Mask\")\n",
        "    axs[count][1].imshow(mask)\n",
        "\n",
        "    # Obtain the predicted mask for the image \n",
        "    predicted_mask = np.asarray(df_pred.predicted_mask[i])[0].squeeze().round()\n",
        "    axs[count][2].title.set_text(\"AI Predicted Mask\")\n",
        "    axs[count][2].imshow(predicted_mask)\n",
        "    \n",
        "    # Apply the mask to the image 'mask==255'\n",
        "    img[mask == 255] = (255, 0, 0)\n",
        "    axs[count][3].title.set_text(\"MRI with Original Mask (Ground Truth)\")\n",
        "    axs[count][3].imshow(img)\n",
        "\n",
        "    img_ = io.imread(df_pred.image_path[i])\n",
        "    img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
        "    img_[predicted_mask == 1] = (0, 255, 0)\n",
        "    axs[count][4].title.set_text(\"MRI with AI Predicted Mask\")\n",
        "    axs[count][4].imshow(img_)\n",
        "    count += 1\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nYB8PloJx79"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}